diff --git a/docs/채팅_QnA_안정화_최적화_보고서.md b/docs/채팅_QnA_안정화_최적화_보고서.md
new file mode 100644
index 0000000000000000000000000000000000000000..22d85ec5fa2b611bb03584ef60ff8513b37859e2
--- /dev/null
+++ b/docs/채팅_QnA_안정화_최적화_보고서.md
@@ -0,0 +1,133 @@
+# 100~200명 동시 접속 대비 채팅/Q&A 안정화 최적화 보고서
+
+## 1. 목표
+- 100~200명 동시 접속 시 채팅과 Q&A 모두 지연 없이 1~2초 내 반영되는 사용자 경험을 유지한다.
+- 메시지 유실 및 중복 수신을 방지하고, Realtime 연결 수를 최소화하여 서버 자원을 안정적으로 유지한다.
+- 부하 테스트 및 모니터링 체계를 마련해 사전 감지·조치 가능한 운영 프로세스를 만든다.
+
+## 2. 현재 구조 요약
+- 프론트엔드: Next.js + React 기반, Supabase Realtime을 통한 실시간 동기화, Optimistic Update로 즉시 UI 반응.
+- 백엔드/DB: Supabase Postgres + Realtime publication, RLS 기반 보안, 메시지 테이블에 webinar/user 기준 인덱스와 created_at 정렬 사용.
+- 주요 경로: `components/webinar/Chat.tsx`에서 Supabase Realtime 채널을 구독하고 `/api/messages/create`로 전송 후 INSERT 이벤트를 수신하여 리스트를 갱신.
+
+## 3. 예상 부하 및 병목 포인트
+- **네트워크**: 다중 채널 가입/해제 반복, 지나치게 많은 메시지 페이로드 전송, 이미지·이모지 등 미니멀 최적화 부족.
+- **DB/Realtime**: `postgres_changes` 이벤트가 과도할 때 row-level payload로 트래픽 증가, 인덱스 누락 시 INSERT/SELECT 지연.
+- **프론트엔드**: 100명 이상에서 불필요한 rerender와 스크롤 관리 미흡 시 FPS 저하, 비효율적인 메시지 병합 로직으로 중복 렌더 가능.
+- **운영**: 연결 실패/지연을 감지하지 못하면 룸 재참여 실패 또는 지연된 메시지가 누락된 것처럼 보이는 문제 발생.
+
+## 4. 최적화 가이드
+### 4.1 Realtime/네트워크
+- **채널 수 최소화**: 동일 웨비나 내 채팅·Q&A를 가능한 한 단일 채널로 묶어 `broadcast` 이벤트 타입과 `postgres_changes`를 함께 사용한다. 이벤트 타입별로 `event: "INSERT"`, `schema: "public"`, `table: "messages"` 등으로 필터링하여 오버헤드를 줄인다.
+- **필드 축소**: Realtime payload는 필요한 필드만 포함하도록 Select 필드 목록을 최소화하고, 프론트엔드에서는 표시용 파생 정보(시간 포맷, 사용자 이름)만 클라이언트에서 후처리한다.
+- **메시지 배치 전송**: 입력 시 클라이언트에서 200~400ms 단위로 배치 큐를 만들어 서버로 보내는 API 호출 횟수를 줄인다(낙관적 업데이트로 UX 유지).
+- **전송량 제어**: 메시지 길이(예: 500자) 및 첨부 제한을 명확히 적용하고, 서버/API 레벨에서 gzip/브로틀리 압축을 활성화하여 이벤트당 바이트 수를 줄인다.
+- **타임아웃 및 재시도**: websocket 연결에 대해 exponential backoff 재연결을 적용하고, 5초 이상 응답 지연 시 "재연결" 배너를 표시하도록 UI 알림을 둔다.
+
+### 4.2 데이터베이스/스키마
+- **인덱스 점검**: `messages(webinar_id, created_at DESC)`, `messages(user_id)`, `messages(hidden)` 복합 인덱스를 유지하여 최신 메시지 조회 및 사용자 필터링을 가속한다.
+- **일괄 조회 한정**: 초기 로딩 시 `LIMIT 50`~`100` + `ORDER BY created_at DESC` 후 클라이언트에서 역순 정렬하여 렌더링, 추가 페이지네이션은 `created_at < last_seen` 커서 기반으로 한다.
+- **RLS 성능**: 정책에 `USING (webinar_id = current_setting('request.jwt_claims.webinar_id')::uuid)` 같은 단순 비교를 사용해 조건을 최소화하고, 정책 함수 호출을 피한다.
+- **Write 패스 최적화**: webhook/함수 대신 DB 트리거 최소화, `NOT NULL`과 `CHECK` 제약을 활용해 애플리케이션 단 validation 비용을 줄인다.
+
+### 4.3 프론트엔드 성능
+- **렌더 최소화**: 메시지 목록을 React.memo/`useMemo`로 감싸고, 리스트는 `maxMessages`(예: 200)까지만 유지하여 오래된 항목을 잘라낸다.
+- **가상 스크롤**: 100명 이상 세션에서는 react-virtualized/react-window 기반 가상 리스트로 DOM 노드를 200개 미만으로 유지한다.
+- **낙관적 업데이트 정합성**: 메시지 ID가 실제 DB id로 치환될 때 이전 낙관적 항목을 키 기준으로 교체해 중복 렌더를 방지하고, 실패 시 취소(roll-back) 토스트를 띄운다.
+- **클라이언트 레이트리미트**: 사용자별 초당 전송 횟수를 3~5회로 제한하고, UI에서 즉시 경고를 제공해 서버 부하와 스팸을 줄인다.
+- **접속 토큰 캐시**: Supabase 세션을 메모리 캐시 후 만료 1~2분 전에 갱신하여 재연결 빈도를 낮춘다.
+
+### 4.4 안정성 및 에러 처리
+- **연결 상태 UI**: 연결/재연결/지연 상태를 배지로 표시하고, `last_seen` 타임스탬프를 저장해 다시 접속 시 누락 메시지를 재요청한다.
+- **중복 방지**: `id` + `created_at` 조합으로 클라이언트 로컬 셋(set)에서 중복을 필터링하고, 동일 내용 반복 시 rate limit 메시지를 안내한다.
+- **에러 로깅**: 채널 join 실패, ACK 지연, message send 실패를 각각 Sentry 로그에 전송하고, 사용자 친화적 메시지(재시도 버튼 포함)를 함께 제공한다.
+
+## 5. 모니터링 및 알림
+- **대시보드**: Supabase Realtime 연결 수, DB CPU/IOPS, 쿼리 지연(P90/P99), 채널별 메시지 초당 처리량을 CloudWatch/Grafana에 시각화한다.
+- **에러 알림**: websocket drop, 5xx API 응답, slow query(>300ms), Realtime deliver 지연(>3초)을 Slack/Email로 알림한다.
+- **사용자 경험 메트릭**: FE에서 전송→수신 소요 시간, 렌더 완료 시간, 스크롤 위치 보존 여부를 web-vitals 형태로 수집한다.
+
+## 6. 부하 테스트 계획 (100~200 동시 접속)
+- **시나리오**: 1) 100명 채팅 전송/읽기 혼합(초당 20~30 메시지), 2) 200명 단순 구독 + 20명 전송, 3) 네트워크 품질 저하(패킷 손실 1~3%, 지연 150~300ms) 환경.
+- **도구**: k6/websocket, artillery, Locust를 활용해 웹소켓·HTTP API 부하를 동시에 생성.
+- **검증 기준**: 전송→수신 지연 P95 2초 이하, 메시지 누락 0건, CPU 70% 이하, 에러율 0.5% 이하.
+- **자동화**: CI에 경량 k6 smoke 테스트(동시 20명)를 배치하고, 배포 전/후 스테이징에서 100명 시뮬레이션을 실행해 비교 리포트를 저장한다.
+
+### 6.1 k6 웹소켓 부하 테스트 실행 절차 (예시)
+1) 테스트 환경 변수 설정 (예: `.env.loadtest`):
+   ```bash
+   SUPABASE_URL="https://<project>.supabase.co"
+   SUPABASE_ANON_KEY="<anon-key>"
+   CHAT_ROOM_ID="<webinar-id>"
+   ```
+2) 샘플 k6 스크립트(`scripts/loadtest/chat_ws.js` 예시) 준비:
+   ```javascript
+   import ws from 'k6/ws';
+   import { check, sleep } from 'k6';
+
+   export const options = {
+     scenarios: {
+       chat_mix: {
+         executor: 'constant-vus',
+         vus: __ENV.VUS || 100,
+         duration: '5m',
+       },
+     },
+     thresholds: {
+       ws_session_duration: ['p(95)<2000'],
+       checks: ['rate>0.99'],
+     },
+   };
+
+   export default function () {
+     const url = `${__ENV.SUPABASE_URL.replace('https', 'wss')}/realtime/v1/websocket?apikey=${__ENV.SUPABASE_ANON_KEY}`;
+     const res = ws.connect(url, {}, function (socket) {
+       socket.on('open', () => {
+         socket.send(JSON.stringify({
+           topic: `realtime:public:messages:webinar_id=eq.${__ENV.CHAT_ROOM_ID}`,
+           event: 'phx_join',
+           payload: {},
+           ref: 1,
+         }));
+
+         // 전송/수신 혼합
+         socket.setInterval(() => {
+           socket.send(JSON.stringify({ event: 'broadcast', topic: 'chat', payload: { text: 'loadtest' } }));
+         }, 1000 + Math.random() * 500);
+       });
+
+       socket.on('message', (msg) => check(msg, { 'received': (d) => !!d }));
+       socket.on('close', () => socket.close());
+     });
+     check(res, { 'status is 101': (r) => r && r.status === 101 });
+     sleep(1);
+   }
+   ```
+3) 실행 명령 (100명 동시): `VUS=100 k6 run scripts/loadtest/chat_ws.js -e SUPABASE_URL -e SUPABASE_ANON_KEY -e CHAT_ROOM_ID`
+4) 주요 지표 수집: `ws_session_duration`(연결 유지), `received` 체크 성공률, 메시지 수신 지연(로그 타임스탬프 비교), 서버 CPU/메모리.
+5) 결과 기록 템플릿:
+   - 날짜/커밋: YYYY-MM-DD / commit SHA
+   - 설정: VUS=X, duration, 메시지 전송 주기
+   - 성능: P95 지연, 누락/에러 수, CPU/메모리, 비용
+   - 관찰: 드롭/재연결 발생 여부, Realtime 채널 가입 지연 여부
+
+### 6.2 Artillery HTTP + WS 혼합 테스트 절차 (선택)
+- 설정: `config.yml`에 HTTP `/api/messages/create` 호출과 websocket 채널 구독 단계를 함께 정의.
+- 실행: `npx artillery run config.yml -e staging -o reports/chat_mix.json` 후 `artillery report`로 HTML 리포트 생성.
+- 검증: HTTP P95 < 400ms, WS P95 < 2s, 실패율 < 0.5%, Realtime 연결 종료(logout/타임아웃) 없음 확인.
+
+### 6.3 테스트 일정 및 적용
+- 주 1회 스테이징 정기 부하 테스트(100명) 및 배포 전 스모크(20명) 자동 실행.
+- 월 1회 프로덕션 근접 테스트(200명) 후 모니터링 알림 임계치 재조정.
+- 테스트 실패 시: 1) Realtime 채널/인덱스 점검 → 2) FE 렌더 최적화 확인 → 3) 스케일 업/쿼터 상향 조치 후 재검증.
+
+## 7. 운영 체크리스트
+- 배포 전: RLS 정책 및 인덱스 존재 확인, 환경 변수(SUPABASE_URL/KEY)와 Realtime publication 설정 점검.
+- 배포 후: 24시간 모니터링 강화, Realtime 연결 수 급증 시 알림 임계치 재조정.
+- 장애 발생 시: 문제 채널 강제 만료 → 클라이언트 재연결 안내 → 누락 메시지 재동기화(커서 기반 재조회) 순으로 대응.
+- 주간 점검: 메시지 테이블 Vacuum/Analyze, 오래된 메시지(예: 90일) 보관 정책 실행, 비용 모니터링.
+
+## 8. 우선순위 적용 로드맵
+1) 단기(당일): 채널 구조 정리, payload 필드 축소, maxMessages 절단, rate limit 적용.
+2) 단기(1주): 가상 스크롤 도입, 재연결/상태 배지 표시, 모니터링 대시보드 구성.
+3) 중기(2~4주): CI 부하 테스트 파이프라인, 자동 누락 복구 로직(last_seen 재조회), 비용/성능 튜닝 리포트 주간화.
