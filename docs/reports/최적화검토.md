좋아. 아래는 **(1) 코드 기반 진단 보고서 v2**, **(2) 네가 “심층 리서치 취합”이라고 준 개선안 문서(우선순위 재정렬/티켓화)**를 **하나로 합쳐서** 만든 **EventLive “전체 최적화 명세서”**야.
Cursor Agent가 그대로 받아서 작업 티켓으로 쪼개고 실행할 수 있게, 너가 요구한 형식(목적/전제/결정포인트/선택기준/리스크/DoD)으로 썼어.

---

# EventLive 성능·안정성 최적화 전체 명세서 v1.0

## 0. 문서 정보

* 적용 대상: EventLive (Next.js App Router + Supabase Postgres/Auth/Realtime + Vercel Serverless) 
* 기준 근거:

  * “장애 원인 코드 기반 진단 보고서 v2” (팩트/로그/코드 위치) 
  * “개선안 검토 + 우선순위 재정렬 + 실행 티켓” (전략/우선순위/작업 쪼개기) 
* 최우선 원칙:

  * **라이브 경험에 영향 주는 경로(참가자 핫패스)는 O(1)로 얇게**
  * **실시간 화면에서 무거운 재집계/반복 DB 조회 금지**
  * **하드코딩 금지**: 주기, TTL, 폴백 정책 등은 전부 “설정 가능” 구조로(환경변수/설정 테이블/대시보드 설정 등)

---

## 1. 현재 문제 정의 (팩트 요약)

### 1.1 장애 트리거(직접 원인)

* `registrations` 테이블은 복합 PK `(webinar_id, user_id)`인데, `presence/ping`와 `access/track`에서 `.select('id')`를 호출해 **400**을 유발. 
* 더 치명적인 점: 400이 나도 **에러 핸들링 없이 로직이 계속 진행**되어 `registration`이 `undefined`가 되고, “등록 없음”으로 오판 → **자동 등록 INSERT 시도** → 409/추가 쿼리로 증폭.

### 1.2 트래픽 증폭기(부하를 키운 구조)

* 참가자 핫패스:

  * `POST /presence/ping`: **120초 ± 10초** 무한 반복, 1회당 **4~10개 쿼리** 가능 
  * `POST /access/track`: 입장 1회 + **5분마다** 갱신, 1회당 **3~7개 쿼리** 
* 관리자 운영 경로:

  * `/stats/access`: 관리자 모드에서 **5초 폴링**(무한) + 1회당 6~8개 쿼리 
* Realtime 불안정 시 폭탄 가능성:

  * Chat이 Realtime 실패 시 **폴백 폴링(2초±)**로 전환될 수 있고, 최악 케이스에서 요청이 급증 가능 

### 1.3 인증/세션 쪽 병목 징후(스냅샷 근거)

* Auth 로그에서:

  * `/auth/v1/user` 평균 약 **~2초**
  * `/auth/v1/token(refresh_token)` 평균 약 **~7~8초**
  * `refresh_token_already_used` 에러 발생(동시 refresh 경합 가능성) 
* `Possible abuse attempt` 경고도 관측됨 → 공개 엔드포인트 방어(레이트리밋/봇 방지)도 최적화 범위로 포함해야 함 

---

## 2. 최적화 목표

### 2.1 최상위 목표

1. **동일 유형 장애 재발 차단**: 400/409 루프 제거 + 핫패스 로직 단순화
2. **웨비나 트래픽이 ‘다른 웨비나/관리자’까지 죽이지 않게**: DB/Auth/Reatime 각각에 상한(guardrail) 도입
3. **비용 폭탄/갑작스런 응답불가 방지**: 급증 감지 + 자동 디그레이드 모드 + 알림

### 2.2 성능 예산(Performance Budget) 원칙

> 숫자는 하드코딩하지 말고, “최대 허용치”는 환경설정 가능하게 설계한다.

* 참가자 핫패스(`/presence/ping`, `/access/track`)는

  * **DB round-trip 수를 고정 상한으로 제한**
  * 등록 생성 같은 “상태 생성”은 핫패스에서 분리
* 운영(관리자) 폴링은

  * 준실시간(10~30초 이상) + 캐시/집계 기반으로 전환 가능하게

---

## 3. 설계 원칙

### 3.1 핫패스는 O(1)

* “하트비트/핑”의 목적은 **연결 유지/활성 상태 표시**다.
* 핫패스는 **(1) 인증 확인 → (2) 단일 upsert/update → 끝** 구조를 목표로 한다.

### 3.2 멱등성(Idempotency) 우선

* 입장/등록/세션 생성은 동시에 여러 번 요청돼도 **같은 결과로 수렴**해야 한다.
* 409(중복키)는 “오류”가 아니라 “동시성 정상 시나리오”로 흡수되는 구조가 목표다.

### 3.3 오류는 Fail-Fast

* 4xx(특히 400)는 재시도/추가 로직을 태우면 **부하만 늘린다**.
* “에러인데도 다음 단계로 진행하는 흐름”을 전수 차단한다.

### 3.4 디그레이드 모드(서킷 브레이커)

* DB/Auth/Realtime 지연이 커질수록 “더 자주 때리는” 구조는 최악이다.
* 장애 신호가 감지되면:

  * 폴링 간격을 늘리고
  * 일부 기능을 늦추고(예: 채팅 실시간성)
  * 운영(관리자) 폴링부터 줄인다.

---

## 4. 최적화 작업 명세

아래는 “워크스트림(WS) → 티켓” 구조.
각 티켓은 Cursor에서 그대로 이슈로 쪼갤 수 있게 작성.

---

# WS-0. 스키마 불일치/오류 루프 제거 (P0)

## 티켓 0-1: `registrations.id` 참조 전수 제거 + 오류 시 즉시 종료

### 목적

* 400 트리거 제거 + 400 후속 증폭(자동 등록/추가 쿼리) 차단 

### 전제 조건

* `registrations`는 복합 PK이며 `id` 컬럼이 없다. 
* 문제 쿼리는 `presence/ping`, `access/track`에 존재한다. 

### 결정 포인트

* 존재 확인을 **(A) select 최소 컬럼**으로 할지, **(B) 애초에 존재 확인을 없애고 멱등 upsert로 갈지**

### 선택 기준

* 참가자 핫패스는 “존재 확인 read”를 반복할수록 비싸진다 → **가능하면 write 1번으로 수렴**시키는 쪽 선호

### 구현 가이드(기준)

* 모든 `registrations` 조회에서 `.select('id')` 같은 참조가 남지 않도록 전수 검색
* supabase 응답에서 `error`가 존재하면:

  * **즉시 return**
  * 이후 로직(등록 생성/세션 업데이트 등) 진행 금지

### 리스크 체크

* “에러를 무시하고 fallback 로직 진행”이 남아 있으면 같은 유형 재발

### DoD

* Supabase/PostgREST 로그에서 `column registrations.id does not exist` 0건 
* 해당 에러가 발생하더라도 서버는 추가 INSERT/추가 쿼리를 수행하지 않음(흐름상 증명)

---

## 티켓 0-2: 409(중복키) 경합을 “정상 흐름”으로 흡수

### 목적

* 동시 입장/동시 ping 상황에서 registrations insert 경합이 DB 부하를 키우는 것을 차단 

### 전제 조건

* 409 원인은 동일 `(webinar_id, user_id)` 동시 INSERT (에러 코드 23505) 

### 결정 포인트

* (A) DB 레벨 on_conflict/upsert로 흡수
* (B) 애플리케이션 레벨에서 409 처리(재조회) 유지

### 선택 기준

* **DB 레벨 멱등성**이 가장 싸고 단순하다(왕복 수 감소)

### 구현 가이드(기준)

* “등록 생성” 동작은 동일 입력에 대해 **항상 성공 응답으로 수렴**
* 409를 “에러 로깅/재시도 유도”로 취급하지 않기

### 리스크 체크

* 409를 정상 처리한다고 해서 “중복 로그인/중복 등록” 정책이 깨지면 안 됨 (정책은 별도 계층에서)

### DoD

* 피크 타임에 registrations 409 발생률이 유의미하게 감소(또는 409가 떠도 추가 쿼리 없이 종료)

---

# WS-1. 참가자 핫패스(입장/시청) 경량화 (P0~P1)

## 티켓 1-1: `presence/ping`에서 “자동 등록 생성” 제거 또는 완전 격리

### 목적

* ping은 heartbeat다. **상태 생성(등록 생성)**을 분리해 핫패스를 가볍게 만든다.

### 전제 조건

* 현재 ping에서 등록이 없다고 판단하면 자동 등록 insert 시도로 이어질 수 있다(특히 400 발생 시) 

### 결정 포인트

* (A) ping은 “등록자만 업데이트”하고 등록 없으면 403/204로 종료
* (B) ping 안에서 등록을 만들되, 반드시 1회 멱등 upsert로만 처리

### 선택 기준

* 안정성 우선이면 (A)
* UX 때문에 “어떤 경로로 들어와도 자동 등록”이 꼭 필요하면 (B) + 강한 상한/가드 필요

### 구현 가이드(기준)

* ping은 아래 중 하나만 수행:

  * presence last_seen 업데이트
  * (필요시) 세션 heartbeat 업데이트
* “등록/프로필/웹инар”의 반복 조회는 ping에서 제거
* 장애 상황에서 ping이 실패해도 전체 서비스가 무너지지 않게(“필수 기능”이 아닌 “품질 기능”으로 취급)

### 리스크 체크

* 기존 플로우가 “ping이 등록을 만들어주기”에 의존했다면, 입장 플로우 회귀(가입/등록 누락) 가능

### DoD

* `presence/ping` 1회 호출당 DB 작업이 고정 상한(예: 1~2회 write 중심)으로 수렴
* ping 호출이 registrations write를 트리거하지 않음(선택 A일 경우)

---

## 티켓 1-2: `access/track` 역할 재정의(세션/접속 로그로 축소)

### 목적

* track이 “입장 처리 + 등록 확인 + 세션 갱신 + 여러 테이블 조회”를 떠안으면 피크에 취약해진다. 

### 전제 조건

* `/access/track`는 입장+5분 갱신, 3~7쿼리 발생 가능 

### 결정 포인트

* track에서 반드시 필요한 “팩트 기록”이 무엇인지(세션? 접속 로그? 둘 다?)

### 선택 기준

* 라이브 안정성 관점에서:

  * **팩트(로그/세션) 기록**만 남기고
  * 해석/집계는 비동기로 분리

### 구현 가이드(기준)

* track은 “로그/세션 upsert”만 수행
* 등록 생성/확인, 프로필 조회는 track에서 제거 또는 캐시/사전 단계로 이동
* 게스트 허용 경로는 특히 상한(레이트리밋/쿨다운)을 갖춘다 

### DoD

* `/access/track` 호출 1회당 DB 연산이 고정 상한으로 감소
* 입장 피크에서도 track이 DB를 긴 시간 점유하지 않음

---

# WS-2. 참가자 핫패스에서 Vercel 서버리스 제거 (P1)

## 티켓 2-1: "참가자 핫패스"에서 Vercel 서버리스를 빼고, 가능하면 Supabase RPC로 직행

### 목적

* `/presence/ping`, `/access/track` 같은 고빈도 요청에서 **Vercel 함수 실행 + auth.getUser 호출**이 누적되면 Auth/DB 모두에 부담
* 서버리스 hop을 제거하여 지연과 비용 감소

### 전제 조건

* 현재 `presence/ping`은 Next.js API Route를 거쳐 Supabase RPC를 호출
* 클라이언트에서 직접 Supabase RPC 호출 가능 (`webinar_presence_ping` RPC 존재)

### 결정 포인트

* (A) 클라이언트 → Supabase RPC 직접 호출 (서버리스 제거)
* (B) 서버리스 유지하되 최소 로직만 (sendBeacon/특수 정책이 필요한 경우)

### 선택 기준

* **권장: (A) 선택** - 가능한 한 클라이언트에서 직접 RPC 호출
* 서버리스가 꼭 필요한 경우(예: sendBeacon/특수 정책)는 "최소 로직"만 남김

### 구현 가이드(기준)

* 참가자 측 heartbeat는 가능한 한 **클라이언트 → Supabase RPC**로 바로 보내서 서버리스 hop 제거
* `usePresencePing.ts`에서:
  ```typescript
  // ❌ 현재: Next.js API Route 경유
  await fetch(`/api/webinars/${webinarId}/presence/ping`)
  
  // ✅ 수정 후: Supabase RPC 직접 호출
  await supabase.rpc('webinar_presence_ping', {
    _webinar_id: webinarId
  })
  ```
* 서버리스가 필요한 경우(예: 세션 추적)는 최소 로직만 유지

### 리스크 체크

* RPC 호출 시 RLS 정책이 적용되는지 확인 필요
* 클라이언트에서 직접 호출 시 보안 검증이 충분한지 확인

### DoD

* `/auth/v1/user` 호출량이 눈에 띄게 감소(서버리스에서 매번 인증확인하던 패턴 제거)
* 참가자 1인당 "분당 서버 호출수"가 목표치로 내려감(예: 0.5회/분 수준)
* `presence/ping` 응답 시간이 개선됨 (서버리스 hop 제거)

---

# WS-3. Auth/세션 병목 제거 (P1)

## 티켓 3-1: 고빈도 API에서 `/auth/v1/user` 의존도 낮추기

### 목적

* Auth 지연이 커진 상황에서도 웨비나 핫패스가 무너지지 않게 한다. 

### 전제 조건

* Auth 로그에서 `/auth/v1/user` ~2초, `/auth/v1/token` ~7~8초 지연 관측 

### 결정 포인트

* (A) 서버에서 매번 `auth.getUser()` 호출 유지
* (B) 서버에서 JWT를 로컬 검증하고 `sub(user_id)`만 추출(고빈도에 한해)

### 선택 기준

* 고빈도(핑/트랙)는 **B**가 유리
* 단, 보안(서명/만료 검증) 실패 시 즉시 401이 필수

### 구현 가이드(기준)

* “이 엔드포인트가 정말 user email/metadata가 필요한가?”를 먼저 분리
* 필요 없는 엔드포인트는 JWT에서 user_id만 뽑아 처리하도록 설계(구현은 Cursor가)

### 리스크 체크

* JWT 검증 구현 오류는 보안 사고로 직결 → 검증 항목 체크리스트(iss/aud/exp/signature) 강제

### DoD

* 웨비나 진행 중 `/auth/v1/user` 호출량(또는 API 내부 auth.getUser 호출)이 유의미하게 감소
* 인증 실패/만료 처리 정확성 유지

---

## 티켓 2-2: `refresh_token_already_used` 경합 방지(싱글플라이트 refresh)

### 목적

* 여러 탭/동시 요청이 refresh를 동시에 수행해 토큰 경합이 발생하는 문제 제거 

### 전제 조건

* `/auth/v1/token`에서 `refresh_token_already_used` 관측 

### 결정 포인트

* (A) 클라이언트 단에서 refresh를 전역 1개로 직렬화
* (B) 서버에서 세션 전략 변경

### 선택 기준

* 빠른 안정화는 (A)
* 구조적으로는 (A)+(B) 혼합 가능

### 구현 가이드(기준)

* refresh 작업은 “동시에 1개만”
* refresh 실패(특히 이미 사용됨)는 즉시 연쇄 refresh를 때리지 말고 “세션 재동기화/재로그인 유도”로 전환

### DoD

* `refresh_token_already_used` 빈도가 피크 시간대에 연쇄로 발생하지 않음
* 로그인/세션 갱신 체감 속도 개선(정성/정량)

---

# WS-4. Realtime/폴백 폭탄 방지 (P1)

## 티켓 4-1: Chat/QA 폴백 폴링에 서킷 브레이커 도입

### 목적

* Realtime 장애가 곧바로 “전 참가자 2초 폴링”으로 이어져 DB를 태우는 상황 방지 

### 전제 조건

* Chat 폴백 폴링(2초±, 백오프 최대 60초) 구현이 존재 

### 결정 포인트

* 폴백의 기본 간격/백오프/중단 조건(4xx/5xx)
* “탭 visible일 때만” 같은 클라이언트 조건

### 선택 기준

* 장애일수록 서버를 덜 때리게(= backoff가 더 공격적으로 커지게)
* 4xx는 재시도 가치가 낮으므로 빠르게 중단/쿨다운

### 구현 가이드(기준)

* 폴백은 “최후의 수단”이고, 기본 폴링 간격은 설정 가능해야 함
* 서버가 429/5xx면 다음 딜레이를 즉시 상향
* 폴백 활성화 시 UI는 “지연 안내”로 품질 저하를 투명하게

### DoD

* Realtime 강제 실패 테스트에서 DB 요청이 선형/폭발적으로 증가하지 않음 (상한 존재)
* 4xx 에러 발생 시 폴링이 즉시 중단됨
* 서킷 브레이커가 연속 실패 시 폴링을 완전히 중단함
* 폴백이 활성화되더라도 전체 시스템이 생존
* 장애 시 "폴백이 서비스 전체를 죽이지 않도록" 안전장치가 확인됨

---

## 티켓 3-2: “중복 로그인 체크” 주기 재설계(이벤트 기반 + 저빈도 보정)

### 목적

* 5초 주기 체크가 Realtime 이벤트/네트워크를 과도하게 만들지 않게 한다. 

### 전제 조건

* 중복 로그인 체크(Realtime Presence)가 5초마다 동작하는 구조가 존재 

### 결정 포인트

* “입장 시 1회 강한 체크” + “이후 이벤트 기반”으로 바꿀지
* 또는 “보정 폴링”을 30~60초 등으로 늘릴지

### 선택 기준

* 정확도보다 생존 우선: 충돌은 “사용자에게 선택지(끊기/취소)”로 해결 가능
* 빈도는 낮추되, 입장 순간 UX는 지키는 방향

### DoD

* presence 관련 이벤트 수/네트워크 부하가 감소
* 중복 로그인 방지 UX는 유지(입장 시 안내 동작)

---

# WS-5. 관리자 대시보드 경량화 (P1)

## 티켓 5-1: `/stats/access` 폴링을 준실시간 + 설정 가능 구조로

### 목적

* 장애 순간에 관리자 화면이 “추가 부하”로 작동하지 않게 한다.

### 전제 조건

* `/stats/access`는 관리자만 5초 폴링(무한) 

### 결정 포인트

* 주기 증가(10~30초) vs 수동 새로고침 vs Realtime 기반

### 선택 기준

* 운영 화면은 초단위 실시간이 아니어도 된다(특히 장애 시)
* 폴링 주기는 하드코딩 금지(설정화)

### 구현 가이드(기준)

* 관리자 폴링은 “장애 감지 시 자동 감속” 가능해야 함
* 첫 진입은 “요약 데이터 1회”로 끝내고, 상세 목록은 사용자 액션 시 로딩

### DoD

* 대시보드를 열어도 DB/서버 부하 상승이 제한됨
* 폴링 주기 조정이 배포 없이 가능(설정 기반)

---

# WS-5. RLS/DB 최적화 (P2)

## 티켓 5-1: 고빈도 RLS(특히 EXISTS) 정책은 “측정 후 최적화”

### 목적

* 추측 최적화가 아니라 실제 병목을 제거한다.

### 전제 조건

* `messages.scoped_read`, `webinar_live_presence` RLS는 EXISTS 기반으로 비용 가능성이 높음 
* 주요 인덱스는 존재하는 것으로 확인됨 

### 결정 포인트

* 실제 병목이 RLS인지(쿼리 플랜/latency/CPU)
* RLS를 바꿀지 vs 호출 패턴을 바꿀지

### 선택 기준

* 1순위는 호출 수/쿼리 수 줄이기(WS-1/3)
* 그 다음 RLS 최적화(정책 단순화, 인덱스 유도, 정책 분리 등)

### DoD

* 피크 상황에서 해당 테이블 관련 쿼리 p95/p99가 개선됨(측정값)
* 권한 모델이 훼손되지 않음(보안 회귀 테스트)

---

# WS-7. 운영 가드레일/관측성 (P0~P1)

## 티켓 7-1: 400/409/Auth 지연/Realtime 장애 알림 체계

### 목적

* “터지고 나서 아는” 게 아니라 “터지기 전 신호”를 잡는다. 

### 전제 조건

* 400/409 다발, Auth 지연, Possible abuse attempt 경고가 이미 관측됨 

### 구현 가이드(기준)

* 최소 알림 세트:

  * `column registrations.id...` 400 급증
  * registrations 409 급증
  * `/auth/v1/token` latency 급증 + `refresh_token_already_used` 급증
  * Realtime disconnect spike(가능하면)
* 알림 채널은 1개만이라도 “즉시” 도착해야 함(슬랙/디스코드/메일)

### DoD

* 테스트 이벤트로 알림이 정상 동작
* 장애 재현 시(스테이징) 알림이 선행 발생

---

## 티켓 7-2: 공개 API(회원가입/설문 등록 등) 레이트리밋/봇 방지

### 목적

* 트래픽 폭주/봇이 Auth/DB를 먼저 죽이지 않게 한다. 

### 전제 조건

* 공개 엔드포인트 중 권한 체크 없는 API가 존재(문서에 Medium 위험도로 표기) 
* Auth에 “Possible abuse attempt” 경고가 관측됨 

### 구현 가이드(기준)

* IP/디바이스 기반 rate limit
* 실패 응답은 빠르게(초저비용) 반환
* (가능하면) CAPTCHA/Turnstile 등 도입 검토(선택)

### DoD

* 부하 테스트/봇 시뮬레이션에서 Auth/DB 요청이 선형으로 증가하지 않음

---

# WS-7. 부하 테스트/릴리즈 게이팅 (P1~P2)

## 티켓 7-1: “웨비나 표준 부하 시나리오” 정의 + 회귀 테스트

### 목적

* 최적화가 “체감”이 아니라 “재현 가능한 수치”로 검증되게 한다.

### 시나리오(명세)

* S1: 1,000명 입장(2분 내 집중) → 60분 시청 유지
* S2: Realtime 장애 가정 → Chat 폴백 켜짐 → 시스템 생존
* S3: 관리자 대시보드 동시 오픈(운영자 3~5명) + 참가자 1,000명

### 측정 항목

* 400/409 발생률
* 핫패스 API p95/p99
* DB requests/time, Auth requests/time, Realtime 연결/이벤트 수(가능 범위) 

### DoD

* “배포 전 체크리스트”로 자동/반자동 실행 가능
* 기준을 넘으면 배포 중단 또는 디그레이드 모드로 강제

---

# WS-9. 구조적 격리(멀티테넌시 리스크 완화) (P3 옵션)

> 이건 “지금 당장 필수”가 아니라, **웨비나 2개 동시 진행 시 ‘한쪽 터지면 같이 터짐’**을 구조적으로 막는 옵션.

### 목적

* 한 이벤트/클라이언트의 폭주가 다른 이벤트에 영향을 주지 않게 격리

### 옵션

* (A) 같은 Supabase 프로젝트 내 이벤트별 자원 격리(한계 있음)
* (B) 클라이언트(혹은 대형 이벤트) 단위로 **Supabase 프로젝트 분리** + 코드/환경 클론
* (C) 메인(개발/데모)과 프로덕션을 도메인/배포 단위로 분리(예: demo vs modoo)

### 선택 기준

* 운영 규모(동시 진행 이벤트 수, 최대 동접, 고객 수)
* 자동화 가능성(서브도메인/DNS/배포 파이프라인)
* 비용/관리 복잡도

### DoD

* 격리된 환경에서 한쪽 장애가 다른쪽에 영향을 주지 않음(관측)

---

## 5. 최종 우선순위 (권장 실행 순서)

1. **WS-0(스키마 불일치/오류 루프) 먼저**

   * 400 제거 + 400인데 계속 진행하는 구조 차단
2. **WS-1(핫패스 경량화)**

   * ping/track의 “역할 축소 + 멱등성” 
3. **WS-2(Auth 병목) + WS-3(폴백 폭탄 방지)**

   * Auth 지연/refresh 경합/Realtime 폴백을 동시에 방어 
4. **WS-4(관리자 폴링 감속)**

   * 장애 시 운영 화면이 짐이 되지 않게 
5. 측정 기반으로 **WS-5(RLS)** / **WS-7(부하테스트)** / **WS-8(격리)**

---

## 6. 전체 DoD 체크리스트

아래가 충족되면 “최적화 1차 완료”로 판단 가능:

* [ ] `registrations.id` 관련 400 로그 0 
* [ ] 400 발생 시 서버 로직이 **추가 INSERT/추가 쿼리로 진행하지 않음** 
* [ ] 등록 생성은 멱등 처리되어 409가 부하 증폭기로 작동하지 않음 
* [ ] `presence/ping`이 “등록 생성기” 역할을 하지 않음(또는 완전 멱등 upsert 1회로 수렴)
* [ ] Realtime 장애 시에도 Chat 폴백이 DB를 태우지 않음(서킷 브레이커) 
* [ ] Auth refresh 경합(`refresh_token_already_used`)이 연쇄 발생하지 않음 
* [ ] 관리자 폴링 주기는 설정 가능하며, 장애 감지 시 자동 감속/중단 가능 

---

원하면 다음 단계로, 이 명세서를 **Cursor 작업 티켓(Jira 스타일)로 “한 장짜리 백로그”**로 바꿔줄게:

* 티켓별 *변경 범위*, *회귀 테스트 포인트*, *롤백 플랜*까지 포함해서 “실행 순서”가 더 선명해지게.
