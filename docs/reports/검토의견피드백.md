아래는 **지금 업로드한 “장애 원인 코드 기반 진단 보고서 v2”** 내용을 기준으로, *우리(현재 EventLive) 구조에서* “서버가 안 터지는 방향”으로 개선안을 **검토 + 우선순위 재정렬 + Cursor Agent 실행 티켓** 형태로 정리한 것입니다. 

---

## 1) 먼저 결론: 이번 장애를 ‘터뜨린’ 핵심은 2가지였다

### (1) `registrations.id` 스키마 불일치 → 400 폭발 → “에러인데도 계속 진행”으로 추가 DB 작업 유발

* `registrations`는 `(webinar_id, user_id)` 복합 PK이고 `id` 컬럼이 없는데, `presence/ping`과 `access/track`에서 `.select('id')`를 호출하면서 400이 발생했습니다. 
* 더 큰 문제는 **400이 나도 에러 핸들링 없이 로직이 계속 진행되어**, “등록이 없다”로 판단하고 **자동 등록 INSERT까지 시도**하는 증폭 루프가 생긴 점입니다(409까지 유발). 

### (2) 핫패스(presence/ping, access/track)에 “인증 확인 + 등록 확인/생성 + 여러 테이블 조회”가 붙어 있어, 동시접속 시 DB/Auth가 같이 과부하

* 보고서 기준으로 `presence/ping`은 **요청 1회당 4~10회 쿼리**까지도 발생했고, 동접 1,000 가정 시 분당 쿼리 수가 급증합니다. 
* Auth 측에서도 `/auth/v1/user`가 고빈도로 발생하고 평균 응답이 2초 수준으로 느려졌고, refresh token 경합(`refresh_token_already_used`)도 관측되었습니다. 

이 2개가 결합하면 “DB가 느려짐 → Auth도 느려짐 → 재시도/폴백/동시입장으로 더 느려짐”의 전형적인 연쇄가 나옵니다.

---

## 2) 진단 보고서 v2에 적힌 “우선 수정 TOP 5” 검토

보고서의 TOP 5는 방향이 대체로 맞지만, **우선순위/방법론**은 조금 다듬는 게 안전합니다. 

### ① `registrations.id` 참조 제거 ✅ (P0 최우선)

* 동의합니다. 이건 “성능 최적화”가 아니라 **기능 오류(스키마 불일치)**라서, 고치면 400 폭발이 사라집니다. 
* 단, **presence/ping 뿐 아니라 access/track에도 동일 패턴이 확인**되므로 “전수 제거”가 DoD에 포함돼야 합니다. 

### ② 관리자 대시보드 폴링 주기 증가 ✅ (하지만 P1로 내려도 됨)

* 관리자가 5초마다 호출하는 `/stats/access`는 분당 쿼리 수 자체는 크지 않지만(보고서 기준 분당 84쿼리 수준), **장애 상황(전체 DB 포화)에서는 작은 추가부하도 체감 영향이 큼** → 완화는 맞습니다. 
* 다만 이번 장애의 “직접 트리거”는 400/409 + 핫패스였으니, 이건 P1로 두고 **P0에서 핫패스부터 얇게** 만드는 게 더 효과적입니다.

### ③ Presence Ping “캐싱/최적화” ✅ (하지만 방향을 바꾸면 더 좋음)

* 보고서는 “등록 확인 캐싱(60초)” 등을 제안했는데, 저는 **캐싱보다 ‘핫패스에서 등록 확인/자동 등록 자체를 제거’**하는 쪽이 더 안전하다고 봅니다. 
* 캐싱은 “지금 구조를 유지”한 채로 부하를 줄이는 패치인데, 핫패스에 *읽기/쓰기/오류 루프가 섞여 있는 것* 자체가 근본 리스크입니다.

### ④ RLS 정책 최적화 ✅ (P2: 측정 기반으로)

* `webinar_live_presence` RLS가 EXISTS 서브쿼리라 고빈도에서 병목 가능성은 인정되지만, 보고서에서도 인덱스는 이미 존재하고 풀스캔 위험은 낮다고 나옵니다. 
* 즉, **P0/P1에서 핫패스 쿼리 수를 1~2개로 줄인 뒤**, 그 다음 단계에서 *EXPLAIN ANALYZE로 실제 비용이 큰지* 확인하고 최적화하는 게 맞습니다.

### ⑤ presence ping 재시도 로직 추가 ⚠️ (권장 수정: “재시도”가 아니라 “서킷 브레이커/백오프”)

* 보고서엔 “400 에러 시 지수백오프로 최대 2회 재시도”가 있는데, **400/409는 재시도해도 해결되지 않는 종류(구조/동시성/권한)**가 많고, 오히려 트래픽만 늘릴 수 있습니다. 
* 대신 **“4xx면 즉시 중단(또는 긴 쿨다운), 5xx/네트워크면 백오프”**로 나누는 게 안정적입니다. (이건 특히 ‘서버가 터질 때 더 때리지 않는’ 안전장치)

---

# 3) 우리 시스템 기반 “안정 구성을 위한 개선안” 제안

아래는 **바로 적용했을 때 효과/리스크가 가장 좋은 순서(P0→P1→P2)**로 정리했습니다. 각 항목은 “기존 구조 존중 + 하드코딩 금지 + 라이브 안정성 우선” 원칙에 맞춰서 작성합니다.

---

## P0: 오늘 바로 고쳐야 하는 것 (장애 재발 방지)

### P0-1) `registrations.id` 참조 전수 제거 + “에러면 즉시 종료” 가드

**목적**

* 400 폭발과 그로 인한 후속 쿼리/자동등록 시도를 즉시 차단 

**구현 가이드(기준)**

* `registrations`는 복합 키이므로, 존재 확인은 “실제 존재하는 컬럼” 기반으로만 수행
* registration 조회에서 **error가 나오면, 그 아래 로직(자동 등록 포함)을 진행하지 않는 fail-fast**를 기본으로
* `presence/ping`, `access/track`에서 동일 패턴 전수 제거(보고서에 이미 위치가 특정됨) 

**DoD**

* Supabase/PostgREST 로그에서 `column registrations.id does not exist` 0건 
* 400이 발생하더라도 “추가 쿼리/추가 insert”로 이어지는 패턴이 제거됨(조기 종료로 확인)

---

### P0-2) `presence/ping`에서 “자동 등록(insert)” 로직 제거 또는 격리

**목적**

* presence 핫패스가 “가벼운 heartbeat”가 아니라 “등록 생성기”로 변하면서 409/DB 락/경합을 유발하는 구조 제거 

**왜 지금 구조가 위험한가**

* 보고서에 따르면 400 발생 시에도 함수가 계속 진행되며, `if (!registration)`이 항상 true가 되어 자동 등록 INSERT가 추가로 발생합니다. 
* 이 패턴은 “사용자 수 × ping 주기”로 폭증할 수 있는 전형적인 장애 유발기입니다.

**구현 가이드(기준)**

* “등록 생성”은 **입장 시점**(SSR 권한체크/등록 페이지/명시적 register API)에서만
* “presence ping”은 **등록을 만들지 않는다**:

  * 등록자면 presence 업데이트
  * 등록자가 아니면 403/204로 끝
* 즉, 핫패스는 최대한 **O(1)**로 유지

**DoD**

* 장애 시간대와 같은 동시접속 상황에서 `/presence/ping`이 `registrations INSERT`를 발생시키지 않음
* 409(duplicate key) 로그가 급감 

---

### P0-3) 등록 생성 경합(409)을 “오류가 아닌 멱등성(idempotent)”으로 처리

**목적**

* 동시 입장 시 같은 `(webinar_id, user_id)`로 여러 insert가 날아와도 DB가 불필요한 에러/재시도/재조회로 흔들리지 않게 함 

**구현 가이드(기준)**

* “등록 생성”은 **한 번만 성공하면 끝**이므로, 충돌은 시스템적으로 정상 시나리오로 취급
* 멱등성 보장을 DB 단에서 처리(한 번의 write로 끝내고, 중복이면 no-op)

**DoD**

* 동시 입장 부하 테스트 시 registrations 관련 409가 거의 0에 수렴(또는 애플리케이션에서 에러로 취급하지 않아 재조회/추가쿼리 없음) 

---

## P1: 1주 내 구조 안정화 (부하 체질 개선)

### P1-1) “참가자 핫패스”에서 Vercel 서버리스를 빼고, 가능하면 Supabase RPC로 직행

**목적**

* `/presence/ping`, `/access/track` 같은 고빈도 요청에서 **Vercel 함수 실행 + auth.getUser 호출**이 누적되면 Auth/DB 모두에 부담 

**구현 가이드(기준)**

* 참가자 측 heartbeat는 가능한 한 **클라이언트 → Supabase RPC**로 바로 보내서 서버리스 hop 제거
* 서버리스가 꼭 필요한 경우(예: sendBeacon/특수 정책)는 “최소 로직”만 남김

**DoD**

* `/auth/v1/user` 호출량이 눈에 띄게 감소(서버리스에서 매번 인증확인하던 패턴 제거) 
* 참가자 1인당 “분당 서버 호출수”가 목표치로 내려감(예: 0.5회/분 수준)

---

### P1-2) `/access/track`의 역할 재정의: “세션 생성/갱신”을 단일 목적 + 단일 write로 축소

**근거**

* 보고서에서 `/access/track`은 3~7개 쿼리를 유발하며, 게스트도 포함될 수 있습니다. 

**구현 가이드(기준)**

* access/track은 “세션 레코드 1개 upsert + heartbeat 갱신” 정도로 축소
* 등록 확인/프로필 조회/웹инар 조회 같은 읽기 작업은 **입장 SSR 또는 별도 단계에서**
* 세션 갱신도 DB 레벨 throttle(이미 세션 heartbeat RPC가 있는 것으로 보임)을 강제 

**DoD**

* `/access/track` 요청 1회당 DB 작업이 “최대 1~2회” 범위로 제한됨
* 게스트 트래픽에서도 DB 부담이 선형적으로 증가하지 않음

---

### P1-3) “Realtime 장애 시 폴백 폴링”은 **서버를 죽이는 스위치**가 될 수 있으니, 서킷 브레이커를 반드시 추가

**근거**

* 보고서가 계산한 최악 시나리오에서 채팅 폴백 폴링은 1,000명 기준 시간당 180만 요청까지도 가능하다고 되어 있습니다. 
* 실제 장애 시간대에 켜졌는지는 미확정이지만, 켜지는 순간 **DB를 바로 죽일 수 있는 레벨**이라 “예방 조치”가 P1로 필요합니다.

**구현 가이드(기준)**

* Realtime 장애(또는 서버 에러율 상승) 시에는

  * 폴백을 “모두가 2초 폴링”이 아니라
  * “긴 간격(예: 10~60초) + 지수 백오프 + 탭 visible일 때만 + 4xx면 중단”으로 강제
* 최악의 경우 “폴백 비활성화 + UI에 안내(현재 실시간이 불안정)”가 DB를 지키는 선택일 수 있음

**DoD**

* Realtime 강제 차단 테스트에서 DB 요청이 급증하지 않음(상한이 존재)
* 장애 시 “폴백이 서비스 전체를 죽이지 않도록” 안전장치가 확인됨

---

### P1-4) 중복 로그인 체크(Realtime Presence)의 주기/이벤트 설계를 재검토

**근거**

* Realtime 메트릭에서 Presence Events가 높은 편이고(24시간 444,914건), 중복 로그인 체크가 5초 주기라는 정황이 보고서에 있습니다. 

**구현 가이드(기준)**

* 중복 로그인 체크는 “5초마다 트래킹”이 아니라

  * 입장 시 track
  * 변경(sync) 이벤트 수신 기반으로 UI 반영
  * 필요 시 아주 드문 주기(예: 30~60초)로만 보정
    형태로 Realtime 이벤트 자체를 줄임
* Realtime이 불안정해지면 폴백이 켜질 가능성이 올라가므로(잠재 리스크), Realtime 자체 부하도 관리 대상

**DoD**

* Presence Events/Connected Clients 스파이크 빈도가 감소(측정) 
* 중복 로그인 감지 품질은 유지(정확도 저하 없음)

---

## P2: 1달 내 (성능·운영 체계 고도화)

### P2-1) RLS EXISTS 병목은 “측정 후” 최적화

**근거**

* 보고서에서도 `webinar_live_presence` RLS EXISTS가 High 위험이라고 평가했지만, 인덱스는 있어 풀스캔 위험은 낮다고 되어 있어요. 

**구현 가이드(기준)**

* 실제로 병목인지 EXPLAIN ANALYZE/pg_stat_statements로 확인
* 병목이 맞다면, “고빈도 테이블에 대한 RLS 비용”을 줄이는 방향으로(단, 권한 모델은 훼손 금지)

**DoD**

* 피크 시간대 RPC latency가 유의미하게 감소(수치로 증명)

---

### P2-2) 데이터 보관/정리 정책(리텐션) + 크론 실패 알림

* `webinar_live_presence` / `webinar_access_logs` 보관이 무기한이면 장기 운영에서 성능이 서서히 악화될 수 있습니다. 
* 크론 실패는 “조용히 누락”되는 유형이라 운영 알림이 있으면 안전합니다. 

---

# 4) Cursor Agent에게 바로 던질 실행 티켓(복붙용)

아래 3개만 먼저 처리하면 “동일 유형 재발” 가능성이 급격히 줄어듭니다.

---

## 티켓 A: registrations.id 스키마 불일치 제거(P0)

**목적**: `registrations.id` 참조로 인한 400 폭발 제거
**전제**: registrations는 복합 PK, id 컬럼 없음 
**결정 포인트**: 존재 확인을 어떤 컬럼(select)로 할지 / 또는 존재 확인 자체를 제거할지
**선택 기준**: “존재 확인은 가벼운 read 1회” 혹은 “idempotent write 1회”로 단순화
**리스크 체크**: 수정 범위가 `/presence/ping` `/access/track` 등 다수 → 전수 검색 필요 
**DoD**: `column registrations.id does not exist` 로그 0, 400 급감

---

## 티켓 B: presence/ping에서 자동 등록 제거(P0)

**목적**: 핫패스에서 registrations insert를 제거해 409/경합/DB 부하 차단 
**전제**: 등록 생성은 입장/등록 API에서 수행 가능
**결정 포인트**: 등록이 없을 때 ping을 403으로 할지 204로 할지(운영 정책)
**선택 기준**: “ping은 절대 상태 생성하지 않는다(등록 생성 X)”
**리스크 체크**: 기존에 ping이 등록 생성에 의존하던 흐름이 있는지 회귀 테스트 필요
**DoD**: ping 경로에서 registrations write 0, 409 감소 

---

## 티켓 C: 폴백 폴링 서킷 브레이커(P1)

**목적**: Realtime 장애 시 폴백이 DB를 죽이지 못하도록 상한/백오프/중단 기준을 둔다 
**전제**: 폴백은 “정상 시 거의 안 돌고, 비상 시만 최소로 동작”해야 함
**결정 포인트**: 4xx/5xx별 처리(중단 vs 백오프), 최소 폴링 간격 상한
**선택 기준**: 장애일수록 서버를 덜 때리도록(지수 백오프 + 최대 간격)
**DoD**: Realtime 강제 실패 테스트에서 DB 요청이 폭증하지 않음

---

원하시면 다음 단계로, **“P0 변경 후 기대 부하 변화(쿼리 수/분, auth 요청/분) 재산정”**을 이 보고서의 수치(쿼리/요청 추정치) 기반으로 다시 계산해서 **개선 효과를 숫자로** 보여드릴게요. 
