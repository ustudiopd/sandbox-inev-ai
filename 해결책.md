좋은 관찰이에요. **95K 호출**은 “Vercel+Supabase가 잘못”이라기보다, **짧은 주기의 폴링 + CDN 미활용 + 중복 요청**이 겹쳐 만든 비용/리소스 폭탄입니다. 아래는 제안하신 캐시 전략을 포함해, 더 견고하고 저렴한 구조로 다듬는 **우선순위별 액션 플랜**과 **즉시 적용 가능한 패치**입니다.
(참고로 현재 `profiles`의 RLS 순환 참조 이슈도 병행해서 잡아야 전체 성능/안정성이 확보됩니다. 해당 이슈는 오류 리포트에 정리되어 있어요. )

---

## TL;DR (바로 적용할 핵심 5가지)

1. **폴링 최소화·중단**: Realtime(WebSocket) 정상화가 1순위. 폴백 폴링은 **10–20초** 이상으로 늘리고, **탭 비활성/백그라운드·오프라인·숨김 시 폴링 중지**.
2. **CDN 캐시(Edge Caching)**: API 응답에 `Cache-Control: public, s-maxage=1, stale-while-revalidate=9` 적용(제안하신 방향 👍).
3. **ETag/304**: 데이터가 안 바뀌었으면 304로 본문 없이 응답 → **네트워크/CPU 거의 제로**.
4. **탭 중복 제거**: 동일 브라우저 여러 탭에서 **BroadcastChannel/SharedWorker**로 **소켓 1개만 유지**, 나머지는 로컬 브로드캐스트.
5. **프로필/RLS 경량화**: `profiles` 조회를 클라이언트에서 제거하고 **JWT 클레임으로 is_super_admin** 확인. 순환 참조를 일으키는 RLS는 단순화. 

---

## 1) 폴링 대신 푸시(Realtime) 복구가 최선

* 현재 호출 폭증의 1차 원인은 폴링입니다. Supabase Realtime(WS)만 정상화되면 호출 수는 **소켓 유지 1회** 수준으로 떨어집니다.
* 커스텀 도메인/환경변수 이슈로 `wss://`가 막힌 케이스가 흔하니, **클라이언트만 커스텀 도메인**, **서버 키/엣지는 공식 도메인**으로 분기했는지 재점검하세요.
* 폴백은 **SSE(서버센트 이벤트)** → 최후의 폴백으로 **긴 주기 폴링**.

> **권장 주기**: 10–20초 + **가시성 기반**(문서 숨김/백그라운드일 때 정지), 온라인 이벤트 기반 재개
> **백오프**: 에러 시 지수 백오프(10s → 20s → 40s … 최대 60–120s)

---

## 2) Edge 캐싱 + 304 응답으로 호출/CPU 폭감

제안하신 `Cache-Control` 헤더 추가는 **아주 효과적**입니다. 여기에 **ETag/If-None-Match**까지 넣으면, 데이터가 안 바뀐 대부분의 요청을 **304(본문 없음)** 으로 처리하여 네트워크/메모리를 크게 줄입니다.

### 서버 (Route Handler) 패치 예시

```ts
// app/api/webinars/[webinarId]/messages/route.ts
import { NextResponse } from 'next/server'

export const runtime = 'edge' // 가능하다면 Edge로: 콜드스타트/메모리↓

// 변하지 않는 쿼리: select 최소화 + 인덱스(id, created_at) 전제
export async function GET(req: Request, { params }: { params: Promise<{ webinarId: string }> }) {
  const { webinarId } = await params
  const { searchParams } = new URL(req.url)
  const limit = Number(searchParams.get('limit') ?? '20')
  const beforeId = searchParams.get('beforeId')

  // ... supabase admin/server client 준비
  // id 또는 created_at에 적절한 인덱스 필수 (쿼리 계획 확인)
  let query = admin.from('messages')
    .select('id,created_at,role,content,author_id') // 필요한 컬럼만
    .eq('webinar_id', webinarId)
    .eq('hidden', false)
    .order('id', { ascending: false })
    .limit(limit)

  if (beforeId) query = query.lt('id', Number(beforeId))

  const { data = [] } = await query
  const messages = data.reverse()

  // --- ETag 계산(마지막 메시지 id 기준, 필요시 해시로 대체)
  const lastId = messages[messages.length - 1]?.id ?? 'none'
  const etag = `"${webinarId}:${lastId}:${messages.length}"`

  const ifNoneMatch = req.headers.get('if-none-match')
  if (ifNoneMatch === etag) {
    return new NextResponse(null, {
      status: 304,
      headers: {
        'Cache-Control': 'public, s-maxage=1, stale-while-revalidate=9',
        'ETag': etag,
      }
    })
  }

  return NextResponse.json(
    { messages, hasMore: messages.length === limit },
    {
      headers: {
        'Cache-Control': 'public, s-maxage=1, stale-while-revalidate=9',
        'ETag': etag,
      }
    }
  )
}
```

### 클라이언트(폴링/요청) 최적화

```ts
// ETag 재사용: 304면 state 갱신 생략
let etagRef: string | null = null

const fetchMessages = async (isLoadMore = false) => {
  const beforeId = isLoadMore ? oldestId() : null
  const url = `/api/webinars/${webinarId}/messages?limit=20${beforeId ? `&beforeId=${beforeId}` : ''}`

  const res = await fetch(url, {
    headers: etagRef ? { 'If-None-Match': etagRef } : undefined,
    cache: 'no-store' // CDN 캐시는 Response 헤더로 제어
  })

  if (res.status === 304) return // 변화 없음 → 렌더/메모리 낭비 X

  etagRef = res.headers.get('ETag')
  const data = await res.json()
  // ... setMessages(data.messages)
}
```

**효과**

* CDN이 **1초 내 동일 키 요청을 흡수** → 서버 함수 실행 횟수 급감
* 변경 없을 때 **304** → 응답 본문 없음, 전송량/메모리/CPU 모두 절감

---

## 3) 멀티 탭·중복 소켓/요청 제거

동일 브라우저에서 탭이 여러 개 열리면 호출이 **선형 증가**합니다. **브로드캐스트 아키텍처**로 한 탭만 네트워크를 잡고, 나머지는 공유합니다.

```ts
// 예시: BroadcastChannel로 메시지 공유
const ch = new BroadcastChannel(`webinar:${webinarId}`)
ch.onmessage = (e) => setMessages(merge(messages, e.data))

// 네트워크 담당 탭(리더)만 서버 구독/폴링 후 ch.postMessage(newMessages)
// 리더 선출: localStorage + storage 이벤트 or leader-election 라이브러리
```

또는 **SharedWorker/Service Worker**를 써서 **소켓 1개**만 유지하게 할 수 있습니다.

---

## 4) 쿼리/스키마·렌더링 경량화 체크리스트

* **DB 인덱스**:

  * `messages(webinar_id, id)` 복합 인덱스 또는 `created_at` 기반 인덱스
  * “이전 대화 더보기(beforeId)”가 **Index Only Scan**으로 내려가는지 `EXPLAIN ANALYZE`로 확인
* **페이로드 줄이기**: 채팅 리스트 `select`는 꼭 필요한 컬럼만. 큰 필드(이미지/첨부/메타) 분리.
* **가상 스크롤**: 수백/수천 메시지 DOM 렌더링 비용을 **react-virtualized/virtuoso**로 상한.
* **직렬화 비용**: JSON 크기 자체 감소(문자열 압축은 CDN이 처리하지만, 서버 JSON 생성도 CPU를 씀).

---

## 5) RLS/권한 쪽 병목·오류를 함께 해소

현재 `profiles`의 RLS가 **순환 참조**로 500을 유발하고 있어(무한 재귀), 인증 가드/미들웨어에서 추가 호출·재시도·타임아웃을 야기할 수 있습니다. **클라이언트에서 profiles 직접 조회를 제거**하고, **JWT에 `is_super_admin`**을 넣어 확인하는 방식으로 단순화하세요. (오류 리포트에 동일 권고가 있습니다. )

**권장 방향**

* 로그인 시 서버에서 `is_super_admin`을 **JWT 클레임**에 주입
* RLS는 `auth.jwt()`(또는 `request.jwt.claims`)만 참조하도록 구성 (테이블 재조회 금지)
* 미들웨어/가드에서 `profiles` SELECT 제거 → **서버 전용(Admin key)** 또는 **JWT만 사용**

> 이렇게 하면 **인증 경로가 얇아지고**, 캐시/304 전략과 결합될 때 **전체 왕복 수**가 크게 줄어듭니다.

---

## 6) 우선순위 실행 플랜 (오늘 적용할 것 → 이번 주)

**오늘**

1. 메시지 API에 **Cache-Control + ETag/304** 적용(위 패치 그대로).
2. 폴링을 **10–20초**로 상향, **가시성 기반 정지 + 지수 백오프** 추가.
3. 클라이언트 `profiles` 조회 **즉시 제거** → 서버/JWT로 대체. 

**내일~이번 주**
4. **BroadcastChannel/SharedWorker**로 **중복 소켓/요청 제거**.
5. 메시지 쿼리 **인덱스 검증/튜닝** + **필드 최소화**.
6. Realtime(WS) 정상화 → **폴링 완전 제거**.
7. 가상 스크롤 적용(렌더링 최적화).

**선택 옵션(트래픽↑시)**

* `/messages`를 **Edge Runtime**로 이관(가능한 경우) → 콜드스타트·메모리 감소.
* “새 메시지 여부만” 확인하는 **경량 엔드포인트** 추가(ETag/Last-Modified 기반, 본문 0).

---

## “Vercel+Supabase가 답이 아닐 수도?”에 대한 판단

* **지금 상태**: 캐시/304/탭중복 제거만으로도 **호출·CPU·메모리 급감**합니다.
* **규모가 더 커질 때**: 장기적으로는 **WS 100% 전환**, 필요시 **전용 WebSocket 게이트웨이(Pusher/Ably/Supabase Realtime 전용 프로젝트)**, **Edge + KV/Cache** 조합을 검토.
* **즉시 갈아엎을 필요는 없습니다.** 현재 병목은 **패턴(폴링/중복/미캐시)**이지 스택 자체의 한계가 아닙니다.

---

## 마지막 점검 포인트

* Vercel Observability에 **ETag 히트율**, **304 비율**, **함수 호출수/Active CPU** 추이를 지표로 등록
* 요청 헤더에 `x-client-id`, `x-poll-interval`, `x-tab-id`를 넣어 **중복·루프** 탐지
* **P75 Duration**/메모리 사용량이 캐시 적용 후 즉시 내려가는지 확인

---

필요하시면 위 패치를 **당장 적용 가능한 PR 형태**로 정리해 드릴게요. 또한, RLS 순환 이슈(슈퍼어드민 판별)를 **JWT 클레임 기반으로 이관**하는 SQL/마이그레이션 스크립트도 함께 제공합니다. (오류 리포트의 권고와 동일한 방향입니다. )
